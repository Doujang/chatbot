

# CommuneChatbot 0.2 版项目介绍

[//]: # (@stageName intro)

很高兴为您介绍 CommuneChatbot 项目!

[//]: # (@info)

我将按自己的思路来一步步介绍这个项目，您也可以随时和我说话。
作为一个Demo，现在掌握的对话能力还非常有限，请您多包涵。

[//]: # (@askNext)

## 什么是 CommuneChatbot

[//]: # (@stageName what_is_commune)

CommuneChatbot 是一个开源的对话机器人项目, 其中 "Commune" 就是 "亲密交谈" 的意思.

这个项目旨在验证作者在对话机器人领域的技术思路和产品设想.
去年底开发了 0.1 版本, 您现在看到的是 0.2 版.

[//]: # (@info)

v0.1 版首先实现了一个可跨平台、跨形式（文本、语音、操作、多媒体等）的对话机器人开发框架,
然后重点验证了 ```复杂多轮对话管理``` 的技术思路, 实现名为 "Host" 的多轮对话管理引擎.

[//]: # (@info)

v0.2 版在此基础上, 进一步探索了更复杂的产品形态和技术方案。
您可以听我逐个介绍产品方面的思路, 或者直接跳到技术部分：

[//]: # (@askNext)
[//]: # (@routeToStage v2_technology_verify)

## v0.2 的产品思路

[//]: # (@stageName production_example)
[//]: # (@stageDesc 产品思路探索)

0.2 版探索的产品思路有：

- 对话式视频
- 聊客 (chatlog)
- 基于 Markdown 的对话式创作

请容我一项一项为您介绍：

[//]: # (@askNext)

### 对话式视频

[//]: # (@stageName convo_video_app)
[//]: # (@intentKeywords 对话式视频)

```对话式视频``` 是一种产品设想，把我们现在很熟悉的 "小视频" 用复杂多轮对话系统组织起来。
形成一种 "对话" + "视频" 的交互体验。

[//]: # (@info)

这种交互形式已经广泛出现在科幻题材文艺作品中。
例如《底特律：变人》开场的接待员，《超人》里克拉克和他父母的影像对话，或是《守望者》里黑人受难资料库的对话式查询界面。

[//]: # (@break)

我认为现有的技术早已经能实现这种产品形态，而且很可能几年之内就会出现在各种行业。
它主要适合机器人主导的，```半开放域``` 对话场景，例如：

- 知识宣讲（公开课）
- 商品展示
- 购物引导
- 陌生人社交
- 知识问答
- 招聘
- 明星偶像产业

[//]: # (@info)

这种产品形态，在不久的未来很可能就出现在淘宝、贝壳找房、微信公众号、Boss直聘、瓜子二手车、BiliBili（已经有互动视频了）、短视频、在线教育等各个我们熟悉的应用上。

[//]: # (@info)

很明显，您现在看到的这个对话机器人，就是对这种思路的一个初步尝试。

[//]: # (@break)

许多人会误解对话式视频和 AI 实现的虚拟机器人。
总认为这种产品需要人工智能技术全面发展，能控制虚拟形象、声音、和真正智能的对话能力才有可能。

[//]: # (@info)

我的产品设想和这种思路最本质的区别在于，后者的目标是取代人，所以才需要追求未来技术；
而我说的对话视频产品，本质上和抖音、知乎、BiliBili 一样，是帮助现在的人自己创造和呈现的工具。

[//]: # (@break)

AI 技术的未来发展或许能把人复活，但真正让人永生的其实是人本身的创作（数据生产）。
而且 AI 也不是永生的载体，算法模拟的人格对算法而言只是一堆数据。
人类在实现肉体永生之前，只可能通过自己创造的东西，永生于其他人类的想象和记忆中。
所以创造才是第一位的。


[//]: # (@askNext)

### 聊客 (chatlog)

[//]: # (@stageName chat_log)
[//]: # (@intentKeywords chatlog,聊客)

和对话式视频一样，从 "半开放域复杂多轮对话机器人" 出发，我设想了 "聊客"（chatlog）这样的产品。

[//]: # (@info)

简单而言，它就像博客（weblog）、播客（videolog）、声客（audiolog）一样，是一种个人知识生产与对外展示的应用形态。

而聊客最大的特点在于，它可以用对话的形式来创作，并且用对话机器人的形式展示给他人。

[//]: # (@break)

您可以设想，在类似于微信的聊天界面中，一段段输出视频、文字、语音，然后自动生成一个结构化的多轮对话的 "文章"。

[//]: # (@info)

而用户也通过对话——而非时间线——的形式，像探索迷宫似的遨游于各种碎片化但成体系的知识内容中。
可以通过提问、交谈的形式跳转到对话机器人已学习的任何知识点上。

[//]: # (@break)

您现在对话的这个机器人，可能就是 "聊客" 的早期尝试了。

[//]: # (@info)

您随时可以尝试跳出机器人的指引，和我进行各种对话。
而我对您的回复能够让机器人立刻学会，从而未来自动掌握新的对话能力。

[//]: # (@askNext)

### 基于 Markdown 的对话式创作

[//]: # (@stageName markdown_wiki)

您现在与之对话的这个机器人，就是作者用 Markdown 的形式编写的。
它与编写一篇文章非常接近，只是可以进一步地使用```[//]: # (@break)``` 这样的注解来教机器人更复杂的对话行为。

[//]: # (@break)

所谓 "半开放域" 对话场景，最大的特点就是核心内容由服务端来主导，伴随开放性的长尾对话 （例如问答、评论）。
而服务端主导的对话流程，绝大多数都可以用 "树" 来描述。因此 markdown 就成为了绝佳的对话机器人书写形式。

[//]: # (@info)

基于对 Markdown 树形结构的解析，我们可以设置多轮对话的默认流向，可以是线性的（例如当前的对话, 相当于正序遍历），也可以是分枝式的，也可以是自定义更复杂的规则。

[//]: # (@break)

进一步的，其实现在网络上绝大多数的知识（文章）都用类似 markdown 这样结构严谨的文档来编写。
我可以把某个技术项目的说明文档（vue，swoole等），或者类似 [JavaGuide](https://github.com/Snailclimb/JavaGuide) 这样的技术笔记直接转化为多轮对话。

[//]: # (@info)

再结合 全文搜索 + 自然语言识别 的技术让用户通过对话快速定位到知识节点上。
并用一个可以逐步教学成长的 问答/闲聊 系统来承担长尾的应答能力。
这样就是一个完整的半开放域解决方案了。

[//]: # (@break)

显然这种偏工程的做法，和基于自然语言技术的做法——将文本分析为更致密的知识单元，通过知识图谱组织起来，并在知识图谱基础上构建对话——并不一样。

[//]: # (@info)

两种方案，技术上其实是一致的，
等我定义知识图谱的模块（用面向对象的思路来结合知识图谱定义多轮对话）完成后, 也会重新定义 markdown 的解析器。

[//]: # (@break)

而最本质的区别仍然在产品形态上，如同对话式视频，
我目前想实现的并不是如何用 AI 在某个知识领域取代人，而是怎么样更好地帮助人创作和呈现。

[//]: # (@info)

做 markdown 方案除了是多轮对话的撰写工具，最终目的还是建立一个对话式的 wiki。
我希望不久之后，自己创造的对话机器人能成为很好的助手，能用一个无限耳机，随时帮我记录、管理、提示各种想法和资讯。


## v0.2 的技术方案验证

[//]: # (@stageName v2_technology_verify)
[//]: # (@stageDesc 技术方案验证)

这里开始介绍 v0.2 版在技术上的探索。

(我也尝试更换一种对话的形式，改为分枝式)

### Ghost In Shells

[//]: # (@stageName ghost_in_shells)
[//]: # (@stageDesc 多个设备同构为一个机器人)

这个方案说起来拗口，我个人取了个好玩的名字叫 ```Ghost in Shells``` 架构。
其实就是常见的思路：把单一对话系统的 "多轮对话管理内核"， 变成多个对话或设备公共的 "状态与行为管理中控"（Ghost)

[//]: # (@info)

Ghost 从所有接入设备中获得输入信息，然后把决策信息原路返回，或者广播给所有设备。
而每个设备的输入输出、通信机制（单通、双工、异步等）、消息的模态（文本、语音、视频）都不一样。
于是又有一个 "shell" 层负责不同设备特异性的通讯、服务，将输入信息一致化，将输出信息个性化。

[//]: # (@info)

CommuneChatbot 只是在这个方向上迈出了一小步，现阶段还只是对话机器人层面跨设备的同构。


#### 设备同构不是通讯异构

[//]: # (@stageName ghost_in_shells_not_just_communication)

首先需要澄清，这种思路并不是通讯层面上的异构，——比如微信同时有网页版、客户端、手机版。
它其实需要整合差别很大的工程。

[//]: # (@info)

我举一个例子，同为聊天系统:

- 微信公众号是同步响应机制
- 百度智能音箱是同步响应 （也有双工的极客模式）
- 钉钉群可以调API异步响应
- 网页版是 websocket 双工通讯

将这四种端同构为一个对话机器人，如果在网页版内进行对话，同步客户端如公众号和智能音箱是接受不到的。

[//]: # (@info)

接受不到又不能完全忽略，因为很多指令性的命令、用户身份数据、权限变更等， Shell 仍然要执行。
所以通讯差异只是同构工作早期的一个难题。

#### 设备同构的动机

[//]: # (@stageName ghost_in_shells_activatioin)

为什么要做同构呢？它源于一个很直观的需求：我能不能用无线耳机，口语操作网页、家电、智能玩具等……成为一个真正的语音 OS ？

[//]: # (@info)

到目前来看，主流的对话 OS 方案是基于硬件和操作系统的。比如苹果、小米、华为等，他们掌握了系统层面的接口。
语音 OS 先和系统对接，系统再通过 API 和软件对接。

[//]: # (@info)

智能音箱也是走类似思路，各种软件或智能家居要预先接入系统。所以智能音箱变得越来越 "重" 了，内置操作系统、可以对接物联网设备，进一步从音箱添加触屏，变得像平板和电视了。

[//]: # (@break)

对于对话式 OS 而言，物理的硬件载体和操作系统是可以绕过去的。

每个设备可以通过内建的通信模块，和对应的云端 Shell 进行交互，Shell维护自己的局部状态；
而若干个云端的 Shell，再和统一的状态管理中控 Ghost 对接，分享统一的全局状态，从而形成初步的同构。

[//]: # (@info)

这种常见的分布式系统或许无法解决同步性、一致性、速度要求特别高的场景。但对于对话式交互本身以秒为单位的比特率，应该是够用了。

[//]: # (@break)

单从技术角度设想，未来许多软硬件可能会有自己的云中控（shell），通过技术委员会认可的公共 interface，在用户授权时，接入指定云端状态管理中控（ghost），然后实现同构。

[//]: # (@info)

那时候的人们，只要一个耳机，联通自己和科幻片相似的虚拟助手，就能接入身边的各种智能设备了。电脑、手机、智能音箱，这类硬件、软件、操作系统高度结合的载体未来恐怕会被淘汰掉。


#### CommuneChatbot 的探索

[//]: # (@stageName ghost_in_shells_implements)

我在设计 "Ghost in Shells" 方案时，遭遇和考虑到的各种问题，简单罗列如下：

- ```输入一致化```
- ```输出抽象化```：方便 Shell 建立多模态的输出策略
- ```强制状态同步```：
    - ghost 广播状态变更
    - shell 强制状态变更
- ```Ghost 锁```：防止 Ghost 裂脑
- ```无状态通讯```：不影响中控状态，不用锁的通讯
- ```Ghost 与 Shell 的通讯```
    - 同步广播
    - 同步不广播
    - 异步不广播
    - 异步广播
- ```特异性通讯```：
    - Ghost 内部的异步任务
    - Ghost 内 session 对另一个 session 的(所有 shells 的)通讯
    - shell 对 shell 的直接通讯
    - Ghost 对 shell 的直接通讯
- ```事件广播```：Ghost 全局广播，shell 主动捕捉并执行
- ```指令状态```：Shell 自身的局部状态暴露到 Ghost，可以由其它端触发
    - 指令自动发现
    - 指令集变更
    - 同步状态声明
    - 指令优先匹配与意图响应

[//]: # (@break)

CommuneChatbot v0.2 基于这套技术思路，重构了 v0.1 的所有代码。
现在的架构已经实现了七八成的思路，但到目前只有精力做一个 demo：

用微信公众号控制网页版。现阶段可以代替网页版输入，下一阶段尝试添加一些网页版的指令自动发现。

### 对话式编程

[//]: # (@stageName conversation_programing)

```对话式编程```是我从15年开始想要开发对话系统的一个最初动机。
说的简单一点，是可以用多轮对话教会机器人多轮对话，进一步可以教机器人所有能力。

[//]: # (@info)

需要强调的是，"对话式编程" 完全不是 "自然语言编程"，
它的目标不是用自然语言语法取代高度规范化的编程语言，
而是用 "对话" 这种 "交互形式" 实现 "互动式编程"。

[//]: # (@info)

而且 "对话式编程" 也不是 "对话式控制"，它不仅是教会机器人用新的语法操纵旧的指令，
关键在于让机器人学会新的能力。

#### 什么是互动式编程

[//]: # (@stageName interactive_programing)

对于专业程序员而言，编程主要是一次性书写完整的代码，然后通过单元测试、debug调试或部署测试来确保质量。
过程中程序员不会和解释器进行交互，通常和 IDE 提供的查询、提示功能进行交互。

[//]: # (@info)

许多编程语言都有自己的 console，可以一边写代码一边查看效果。问题在于代码会自动保存，也不具备自我解释能力。

[//]: # (@info)

非专业编程中的互动体验反而多一些，比如给运营人员准备的拖拽式编程，给儿童准备的选择题式编程等。
总得来说，交互式编程不是程序员的单方面输出，解释器具备自我描述、自我展示的能力，能与程序员进行信息交换，提供对代码高级封装后给出来的选择。

[//]: # (@break)

基于图形界面的交互式编程，对于人类接受信息的成本很低，但开发和交互的成本很高。
自然语言是我们人类对外输出信息最为高效的方式。

[//]: # (@info)

当未来我们每个人，上至老人下至小孩，对交互式编程习以为常的时候，一定是以自然语言为主体的。

#### 对话式编程的核心技术

[//]: # (@stageName conversational_programing_core)

我个人认为，自然语言技术对于对话式编程是必要而非充分条件。
工程才是最关键的，需要有```非常严谨的复杂多轮对话管理```，和```双层解释器的架构```。

[//]: # (@info)

而最本质的一点在于，它必须达到通过自身创造出来的逻辑能力，自身立刻加载。
从而在生产效率上达到指数性质的飞跃。
这也是对话式编程和```图形拖拽编程```的本质区别，后者修改的是别的程序的逻辑，而非自身的。


[//]: # (@askNext)

##### 复杂严谨的多轮对话管理

[//]: # (@stageName complex_dialog_manage)

"复杂而严谨" 的多轮对话管理，指的是没有轮数或语料的限制，上下文之间的跳转和回归有丰富的多样性，同时又是非常规则、可预期的。

[//]: # (@info)

就像我们程序员使用 IDE 一样，我们在任何一个界面时所有可做的事情，构成了一个有限指令的封闭域。
而我们的操作会打开各种 "交互" 界面如菜单、对话框、选择、搜索等等。

[//]: # (@info)

当我们下达一个明确的指令比如切换窗口，执行重构，运行测试等；
所有的后续流程、界面变化、信息呈现、场景切换、回归都必须严格符合预期。
不能是一个概率性的结果。

[//]: # (@askNext)

##### 双层解释器架构

[//]: # (@stageName double_explainer)

我个人设计的方案，认为对话式编程需要同时运行两种类似 "解释器" 的模块。

[//]: # (@info)

第一层是对话层解释器，能够把对话中产生的运行逻辑编辑成逻辑树，保存下来。
这一层还需要具备自我描述、解释和提示的功能。

[//]: # (@break)

第二层则是机器人自己的驱动层，它会动态加载预定义的逻辑树，同时实时加载新改动的内容；
通过这一层来驱动自己的行为。

[//]: # (@info)

所以对话式编程一定会有先针对驱动层的元对话编程，然后才能通过对话的形式生产新的逻辑。
它会是在高级编程语言基础上的更高级语言，才能具备交互编程的能力。

[//]: # (@info)

这也是 "对话" 的优势所在，对话新产生的逻辑对话系统能够立刻执行。
这是图形界面暂时难以达到的，难以想象用图形编辑器拖拽出一个图形编辑器的新功能来。
因为程序生产图形的成本，数百倍于文字。


#### 对话式编程的四阶段演进

[//]: # (@stageName conversational_coding_evalution)

我个人设想，对话式编程是一个正在发生中的历史，已经有很多初级形式存在。而它的演进过程可能分为四个阶段。

- 复杂交互管理
- 预定义元对话
- 图灵完备
- 高级封装

[//]: # (@break)

第一个阶段，复杂交互管理：要形成双层解释器的架构，并且能实现通用的交互模型。
例如现在的图形界面拖拽编程，就是建立在成熟的```图形界面交互系统```基础上。

[//]: # (@info)

复杂的交互模型，必须是不限轮次、不依赖资源、上下文跳转严谨的。
在此基础上，还必须进一步实现```并行、异步、阻塞```等各种常见的交互特性。

[//]: # (@break)

第二个阶段, 预定义的元对话：所有早期的交互编程模型，一定是经过严谨设计的固定模型。
比如图形化的拖拽编程界面。

[//]: # (@info)

这个阶段的对话编程系统，可以通过预定义的 "元对话"，生产出新的对话逻辑来。
例如 CommuneChatbot 施工中的一个 Demo，允许用户在对话中制造一个对话式迷宫，一边创造一边调试。

[//]: # (@break)

第三个阶段，图灵完备阶段：对话系统把元对话的层次逐步下探，直到最小的单元，达到编程语言图灵完备的要求。
当然，这个阶段主要需要自然语言理解技术的进步，才能够实现低成本的最小单元表达。

[//]: # (@info)

到这个阶段，意味着第一层对话式的解释器，已经能够编写出第二层驱动器所能实现的一切逻辑。

[//]: # (@info)

这时就正式达到了 "更高级编程语言" 的水平了。实现了 ```二进制编程 -> 语义化编程 -> 跨平台的语义化编程 -> 非编译语言 -> 碎片化语言 -> 碎片化自然语言```  的演进。

[//]: # (@break)

第四个阶段，则是在第三个阶段的高度之上，重新回到第二个阶段的任务。
再去封装高级的 "元对话"，来指数级地降低对话式编程的工作量和复杂度。

[//]: # (@info)

到这个阶段，类似《钢铁侠》中斯塔克与人工智能机器人对话，教会它新的任务和命令，就自然可以实现了。

#### CommuneChatbot 的探索

[//]: # (@stageName conversational_coding_in_commune)

CommuneChatbot 项目在初步地探索对话式编程的第一阶段和第二阶段。

[//]: # (@info)

v0.1 版旨在开发一个上下文严谨的对话管理内核。
v0.2 版做了优化，形成了一套 depend/yield/block/sleep/gc/backtrace/callback/cancel 的上下文栈结构。以保障各种情形下的上下文切换和回归符合预期。

[//]: # (@info)

进一步的，v0.2 实现了四种非阻塞对话机制，包括双工推送、异步非阻塞、多进程对话、同步非阻塞等。
试图实现接近 app、网页应用的复杂交互能力。

[//]: # (@break)

v0.2 版还做了颠覆性的重构，在多轮对话管理逻辑层上搭建了一个配置层，机器人通过读取配置来驱动自己的行为。
从而可以在预定义的元对话中，生产新的对话逻辑配置。然后立刻动态加载，让机器人学会新的能力。

[//]: # (@info)

这相对初级地实现了双层解释器的思路。
而现阶段预定义的元对话，主要用来管理机器人自身的能力，包括用对话教会它新的回复策略，和管理它自身的功能模块。

[//]: # (@break)

开发时设计了好几套元对话能力，包括：

- 创造树状多轮对话的元对话，可以用它来生产文字对话游戏，对话式迷宫等。
- 通过对话生成的问卷调查
- 通过对话生成的 API 调用

[//]: # (@info)

不过元对话需要大量的开发和调试。等我进一步解决了用户乱输入敏感内容的风险后……会把这个模块放出来，作为用户可分享的功能。

### 非阻塞的多任务对话

[//]: # (@stageName none_blocking_conversation)
[//]: # (@stageDesc 非阻塞的多任务对话)

所谓 "非阻塞多任务对话"，指的是用户和机器人进行多个任务对话，不需要等上一个任务全部完成后才能开始下一个任务。
这其实是自然对话的常态，也是其它多轮交互系统（例如图形界面）的基本能力。

[//]: # (@info)

简单而言，当对话系统从闲聊上升成为操作系统时，绝大部分复杂任务都不可能立刻给出结果。比如让机器人去查资料、订票等等。对于一个阻塞的对话系统，就必须等一个任务彻底完成才能继续对话，显然是低效的。

[//]: # (@break)

我一直认为对话系统除了自然语言能力之外，在交互性上也应该对标其它所有的交互系统。
所以 v0.1 版专门设计了多任务对话的解决方案，但苦于当时没有双工通讯场景，无法验证。

[//]: # (@info)

于是 v0.2 干脆自己写了一个双工的前端对话界面，以验证相关技术。

[//]: # (@info)

本项目所强调的 "复杂多轮对话问题"，似乎一直对用户而言不够直观，不容易看出和机器学习方案的差别。
我感到一旦进入 "异步" 的领域，这种差别就非常容易体现了。

#### 双工主动推送

[//]: # (@stageName duplex_push)

双工主动推送是最简单的非阻塞对话。系统把任务交给计划任务模块、事件机制或第三方服务，接受到回调后，通过双工通道主动推送给用户。

[//]: # (@info)

双工技术方案细节很多，但原理上很简单，这里不赘述了。


[//]: # (@break)

CommuneChatbot 在主动推送上的实现更复杂一些。
基于 Ghost in Shells 架构，机器人异构的多个对话平台并非都是双工的，例如微信公众号。

[//]: # (@info)

这时，主动推送消息需要先打包成 delivery 提交给 Ghost，因为 Ghost 掌握了所有的通信渠道，
才能主动广播给所有 shell。再由 shell 自己处理不同端上的异步通知的呈现策略。

[//]: # (@info)

这种解决方案对于没有双工能力的通信通道就很重要。以微信公众号为例，一次响应不能超过 X 毫秒，否则会通知用户系统故障。而很多任务都不可能这么快执行完，就必须要有符合同步对话场景的异步通知能力。

[//]: # (@askNext)

#### 异步非阻塞对话

[//]: # (@stageName async_none_blocking)

单纯双工推送，有几个关键的能力不足，例如：

1. 无法动态了解上下文的记忆，必须一次提交
2. 必须由第三方服务自己组织好所有回复内容

[//]: # (@info)

本质上，是第三方必须耦合对话系统自身的能力，才能实现第三方的主动推送。
因此，需要有对话系统自身的异步对话能力。

[//]: # (@break)

CommuneChatbot 设计了一套 "多进程" 系统。
简单而言与用户的会话（session）可以同时运行多个上下文独立的进程，
只有一个主进程掌握来自用户的输入消息，但所有子进程都可以对外输出。

[//]: # (@info)

因此，主对话进程可以开启一个子进程，让子进程去调用第三方服务，得到返回结果，然后子进程再把结果转化为对话输出给用户。
这样第三方服务就完全和对话系统解耦了。

[//]: # (@info)

就像是我们有个事情交待给机器人，机器人再交待给别人去做；别人做完后告诉机器人，机器人再组织语言后告诉我们。

[//]: # (@askNext)

#### 平行多进程对话

[//]: # (@stageName parallel_none_blocking)

通过多进程来解决异步非阻塞问题，进一步就可以实现平行对话。

[//]: # (@info)

举一个例子，在一个群聊中，每个人都对机器人说话，机器人也可以和每一个人进行多轮对话；而同时，又可以回应公共的对话。

[//]: # (@info)

每个对话的上下文相互之间不冲突，却又共享相同的上下文记忆。类似于我们在一个浏览器里打开很多个窗口，但每个窗口使用者都是同一个人，还可以相互传递数据和状态。

[//]: # (@break)

这个技术方案对我自己而言是重大的突破。让一个机器人在相同的上下文记忆中，同时执行多个复杂多轮对话，对于机器学习实现的多轮对话机器人是难以想象的。

[//]: # (@info)

在这基础上还可以进一步实现对话的多任务调度。用户可以同时给机器人下达多个复杂任务，然后可以按需切换当前任务和机器人对话。

[//]: # (@askNext)

#### 同步非阻塞对话

[//]: # (@stageName sync_none_blocking_convo)

在各种非阻塞对话场景中，同步非阻塞（可以用编程语言的协程来理解）是最难以实现的。而且我认为这是最好体现工程手段必要性的功能，机器学习手段无法脱离工程去实现它。

[//]: # (@info)

这个概念不太好理解。举个简单的例子，我和一个餐馆里的对话机器人点菜，点完菜到上菜还有很长一段时间。
这时，对话机器人可以和我闲聊，包括推荐其它菜品，告诉我最近的营销活动等。这就是 "非阻塞"。

[//]: # (@break)

等到菜做好了，我还正在和机器人聊天，机器人会打断我，问我菜已经做好现在要不要上菜。
这种 "打断"，重新占据话题，就是回归 "同步"。

[//]: # (@info)

而等我告诉机器人 "现在就上菜" 之后，机器人会继续之前被打断的话题聊下去。上下文保持连贯。

[//]: # (@break)

CommuneChatbot 已经实装了这套机制。一个最直观的例子，就是我可以在微信的客户端申请加入网页版的某个会话，然后网页版的用户通过之后，我就接到通知正式加入对话。

[//]: # (@info)

整个过程遇到等待环节，都不会阻塞，而是可以进行别的对话。到了必须响应的环节，就会打断当前对话。而完成响应之后，又可以继续之前被打断的对话。

[//]: # (@break)

CommuneChatbot 实现同步非阻塞的原理有两部分。
首先，可以想象一个需要非阻塞执行（yield）的任务，像一个皮球一样交给了第三方。
第三方完成任务之后，又把这个皮球交回来。这是语境传递 （Context yielding）原理。

[//]: # (@info)

对话中断、进入非阻塞时，会在对话进程中生成 yielding 栈，保留上下文关系。
而当异步任务执行完，则会与当前进行中的对话进行优先级的比较 （challenge），输的一方进入 blocking 栈，
等待机会再夺取对话主导权。


## 结束语

[//]: # (@stageName ending)

以上就是对 CommuneChatbot v0.2 版产品思路和技术验证的介绍了。感谢您付出的时间！

[//]: # (@info)

这个对话本身就是一个综合性的 demo，集合了 视频对话、markdown 编辑、对话学习等各方面的能力。
希望能把这个项目的技术思路给您一个初步的音箱。

[//]: # (@info)

这只是一个用几天时间完成的 Demo，各种不足请多多包涵，也请多多指教。欢迎随时对机器人留言，我会用教学系统完善机器人的回复能力。

接下来我也会开发更多的示例来完善各种功能的呈现。

[//]: # (@break)

结束，再见！




